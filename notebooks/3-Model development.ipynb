{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# lib_src = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "lib_src = '\\\\Users\\\\mauricio\\\\gdrive\\\\python\\\\machine-learning'\n",
    "sys.path.insert(1, lib_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dummies import get_dummies_indices\n",
    "from metrics.log_loss import multi_multi_log_loss\n",
    "from preprocessing.combine_text_columns import combine_text_columns\n",
    "from model_selection.multilabel import multilabel_sample_dataframe, multilabel_train_test_split\n",
    "from size import size\n",
    "from to_csv_to_zip import to_csv_to_zip\n",
    "from fit_cache import fit_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "import psutil\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, randomize, optimize category type and get dummy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dir = '/Users/mauricio/Documents/predictions/'\n",
    "model_dir = '/Users/mauricio/Documents/models/'\n",
    "df = pd.read_csv('../data/TrainingData.csv', index_col=0)\n",
    "df = df.sample(frac=1, random_state=1)\n",
    "LABELS = ['Function', 'Object_Type', 'Operating_Status', 'Position_Type', 'Pre_K', 'Reporting', 'Sharing', 'Student_Type', 'Use']\n",
    "FEATURES = [feature for feature in df.columns if feature not in LABELS]\n",
    "NUMERIC_FEATURES = ['FTE', 'Total']\n",
    "TEXT_FEATURES = [text_feature for text_feature in FEATURES if text_feature not in NUMERIC_FEATURES]\n",
    "df[LABELS] = df[LABELS].apply(lambda x: x.astype('category'), axis=0)\n",
    "y = pd.get_dummies(df[LABELS], prefix_sep='__')\n",
    "holdout = pd.read_csv('../data/TestData.csv', index_col=0, dtype={'Facility_or_Department':'object', 'Text_4':'object'})\n",
    "holdout = holdout[FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model metric: multi-multi log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "cci = get_dummies_indices(df[LABELS])\n",
    "multi_multi_log_loss_scorer = make_scorer(multi_multi_log_loss, greater_is_better=False, needs_proba=True,\n",
    "                                          class_column_indices=cci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "#from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.get_normalized_total import get_normalized_total\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False,\n",
    "                                    kw_args = {'to_drop': NUMERIC_FEATURES + LABELS})\n",
    "get_numeric_data = FunctionTransformer(get_normalized_total, validate=False,\n",
    "                                       kw_args = {'reference':'FTE', 'total':'Total'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "            ('numeric_selector', get_numeric_data),\n",
    "            ('imputer', SimpleImputer(strategy = 'constant', fill_value = 0))\n",
    "        ], verbose=True)),\n",
    "        ('text_features' , Pipeline([\n",
    "            ('text_selector', get_text_data),\n",
    "            ('vectorizer', HashingVectorizer(norm = None, binary = False, alternate_sign = False, dtype = 'uint8')),\n",
    "            ('reducer', SelectKBest(score_func = chi2))\n",
    "        ], verbose=True))\n",
    "    ])),\n",
    "    ('interactor', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "    ('scaler', MaxAbsScaler()),\n",
    "    ('classifier', OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection.learning_curve import plot_learning_curve\n",
    "from model_selection.validation_curve import plot_validation_curve\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_sizes = np.linspace(0.2, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves help comparing algorithms for bias-variance behavior, choosing model parameters during design, adjusting optimization to improve convergence and determining the amount of data used for training.\n",
    "\n",
    "In this pipeline, the amount of data is determined not only by the sample size, but also by:\n",
    "* the feature generation step `HashingVectorizer` with its parameter `ngram_range`.\n",
    "* the feature selection step `SelectKBest(chi2)` with its parameter `k`.\n",
    "* the feature generation step `PolynomialFeatures` with its parameters fixed.\n",
    "\n",
    "Initially, it's picked main classification algorithms that support specific characteristics of the pipeline:\n",
    "* Sparse data produced by the `HashingVectorizer` \n",
    "* Negative numbers present in the numeric features `Total`and `FTE`.\n",
    "\n",
    "Because of this characteristics, alghorithms such as Naive Bayes can't be used:\n",
    "* `GaussianNB` requires dense data\n",
    "* `MultinomialNB` requires positive numbers.\n",
    "\n",
    "Other alghorithms were much slow hence it's limited to `LogisticRegression`.  This limitation is expected to be removed in a future version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train sizes is a sligthly variation of the [default](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve) `np.linspace(0.1, 1.0, 5)` to keep evenly spaced intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = multilabel_sample_dataframe(df, y, size=0.01, min_count=7, seed=1)\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS], prefix_sep='__')\n",
    "print('Sample size:', sampling.shape[0])                                    # 4002\n",
    "print('Train sizes:', (sampling.shape[0] * train_sizes * .8).astype('int')) # [640 1280 1920 2561 3201]\n",
    "print('Test sizes :', (sampling.shape[0] * train_sizes * .2).astype('int')) # [160 320 480 640 800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [#('Dummy (uniform)', OneVsRestClassifier(DummyClassifier(strategy='uniform'))),\n",
    "               ('Logistic regression', OneVsRestClassifier(LogisticRegression(solver='liblinear')))#,\n",
    "               #('SVC', OneVsRestClassifier(SVC(probability=True))),\n",
    "               #('xgboost', OneVsRestClassifier(XGBClassifier())),\n",
    "               #('GBM', OneVsRestClassifier(GradientBoostingClassifier())),  # mostly slower/worse thanxgboost\n",
    "               #('Random Forest', OneVsRestClassifier(RandomForestClassifier())),\n",
    "               #('K-Neighbors', OneVsRestClassifier(KNeighborsClassifier()))\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the`k` parameter space limits for each `ngram_range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_kmax = [len(CountVectorizer(ngram_range=(1,n),\n",
    "                                  dtype='uint8').fit(combine_text_columns(sampling)).vocabulary_)\n",
    "              for n in range(1, 4)]  # [1546, 8703, 19981]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ngram1_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[0])), 7).round().astype('int')\n",
    "ngram1_logspace = np.hstack((ngram1_logspace[ngram1_logspace < ngram_kmax[0]], ngram_kmax[0]))\n",
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,1)],\n",
    "                             'union__text_features__reducer__k': ngram1_logspace}]) # [316, 562, 1000, 1546]\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=-1, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ran only with `n_jobs=None` for the `plot_learning_curve()` and `n_jobs=2` for the pipelines's classifier step `OneVsRestClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngram2_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[1])), 7).round().astype('int')\n",
    "ngram2_logspace = np.hstack((ngram2_logspace[ngram2_logspace < ngram_kmax[1]], ngram_kmax[1]))\n",
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,2)],\n",
    "                             'union__text_features__reducer__k': [316, 562, 1000, 1778, 3162]}]) # ngram2_logspace}])\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=2, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ran only with `n_jobs=None` for the `plot_learning_curve()` and the pipelines's classifier step `OneVsRestClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,2)],\n",
    "                             'union__text_features__reducer__k': [5623]}]) # ngram2_logspace}])\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=None, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Didn't run even with `n_jobs=None` for the `plot_learning_curve()` and the pipelines's classifier step `OneVsRestClassifier()` due to lack of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,2)],\n",
    "                             'union__text_features__reducer__k': [8703]}]) # ngram2_logspace}])\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=None, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3-gram (lacks memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngram3_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[2])), 11).round().astype('int')\n",
    "ngram3_logspace = np.hstack((ngram3_logspace[ngram3_logspace < ngram_kmax[2]], ngram_kmax[2]))\n",
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,3)],\n",
    "                             'union__text_features__reducer__k': ngram3_logspace}])  # \n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=None, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "| sample | min | jobs | features | ngram | train_sizes |    k | interact | logloss|      time |     peak|\n",
    "| -----: | --: | ---: | -------: | ----: | ----------: | ---: | -------: | ------:| --------: |  ------:|\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  316 |    51681 | 0.3151 |   5.9 min | 2.2 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  562 |   159330 | 0.2941 |   9.1 min | 2.2 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | **(1,1)** | (0.2, 1, 5) | **1000** |   **505515** | **0.2888** |  16.9 min | 2.3 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 1546 |  1203576 | 0.3159 |  31.1 min | 2.3 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,2) | (0.2, 1, 5) |  316 |    51681 | 0.3880 |   6.4 min | 2.3 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,2) | (0.2, 1, 5) |  562 |   159330 | 0.3438 |  14.7 min | 2.2 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,2) | (0.2, 1, 5) | 1000 |   505515 | 0.3210 |  33.6 min | 2.2 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,2) | (0.2, 1, 5) | 1778 |  1590436 | 0.3139 | 102.4 min | 2.3 GiB |\n",
    "|   0.01 |   7 |    4 |  2\\*\\*20 | (1,2) | (0.2, 1, 5) | 3162 |  5016528 | 0.3098 | 228.6 min | 4.6 GiB |\n",
    "|   0.01 |   7 |    1 |  2\\*\\*20 | (1,2) | (0.2, 1, 5) | 5623 | 15840006 | 0.3250 | 589.6 min |  14 GiB |\n",
    "|   0.01 |   7 |    1 |  2\\*\\*20 | (1,2) | (0.2, 1, 5) | 8703 | 37918986 | |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best is `k=1000` and `ngram_range=(1,1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization validation curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is the inverse of regularization strength, therefore, greater values specify weaker regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1000)\n",
    "param_name='classifier__estimator__C'\n",
    "param_range = np.logspace(-0.5, 0.5, 5)  # [0.31622777, 0.56234133, 1, 1.77827941, 3.16227766]\n",
    "param_label = 'C'\n",
    "plot_validation_curve(pl, sampling[FEATURES], dummy_labels, param_name, param_range, param_label,\n",
    "                      cv=5, scoring=multi_multi_log_loss_scorer, n_jobs=-1, verbose=11, xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding fixed `ngram_range=(1,1)`, `n_features=2**20` and `k=1000`, the best is `C=1`, scoring `0.2888`, elapsed `25.4 minutes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "              classifier__n_jobs = -1,\n",
    "              classifier__estimator__C = 1,\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1000)\n",
    "\n",
    "model_name = '0.01-k1000-logistic-regression-C1'\n",
    "pl = fit_cache(pl, sampling[FEATURES], dummy_labels, model_dir, model_name)\n",
    "to_csv_to_zip(prediction_dir, model_name, pl.predict_proba(holdout),\n",
    "              holdout.index, dummy_labels.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ngram_range=(1,1)`, `n_features=2**20`, `k=1000` and `C=1`, elapsed `5.1 minutes`, DrivenData score: `0.5598`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# 10% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 40027\n",
      "Train sizes: [ 6404 12808 19212 25617 32021]\n",
      "Test sizes : [1601 3202 4803 6404 8005]\n"
     ]
    }
   ],
   "source": [
    "sampling = multilabel_sample_dataframe(df, y, size=0.1, min_count=2, seed=1)\n",
    "print('Sample size:', sampling.shape[0])                                     # 40027\n",
    "print('Train sizes:', (sampling.shape[0] * train_sizes * .8).astype('int'))  # [6404 12808 19212 25617 32021]\n",
    "print('Test sizes :', (sampling.shape[0] * train_sizes * .2).astype('int'))  # [1601 3202 4803 6404 8005]\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS], prefix_sep='__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('Logistic regression (liblinear)', OneVsRestClassifier(LogisticRegression(solver='liblinear')))#, #fast4small\n",
    "    #('Logistic regression (lbfgs)', OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=200))), #fast4small\n",
    "    #('Logistic regression (sag)', OneVsRestClassifier(LogisticRegression(solver='sag', max_iter=4000)))#, # faster?\n",
    "    #('Logistic regression (saga)', OneVsRestClassifier(LogisticRegression(solver='saga', max_iter=3200))), #fast??\n",
    "    #('Logistic regression (newton-cg)', OneVsRestClassifier(LogisticRegression(solver='newton-cg')))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the`k` parameter space limits for each `ngram_range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_kmax = [len(CountVectorizer(ngram_range=(1,n),\n",
    "                                  dtype='uint8').fit(combine_text_columns(sampling)).vocabulary_)\n",
    "              for n in range(1, 4)] # [2579, 18282, 46898]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ngram1_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[0])), 7).round().astype('int')\n",
    "ngram1_logspace = np.hstack((ngram1_logspace[ngram1_logspace < ngram_kmax[0]], ngram_kmax[0]))\n",
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,1)],\n",
    "                             'union__text_features__reducer__k': ngram1_logspace}]) # [316, 562, 1000, 1778, 2579]\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=-1, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram **(TORUN sometime)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[1])), 7).round().astype('int')\n",
    "ngram2_logspace = np.hstack((ngram2_logspace[ngram2_logspace < ngram_kmax[1]], ngram_kmax[1]))\n",
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,2)],\n",
    "                             'union__text_features__reducer__k': ngram2_logspace}]) # ???\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=None, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| sample | min | jobs | features | ngram | train_sizes |    k | interact | logloss|      time |      peak |\n",
    "| -----: | --: | ---: | -------: | ----: | ----------: | ---: | -------: | -----: |  -------: | --------: |\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  316 |    51681 | 0.1333 |  38.6 min |    2.2 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  562 |   161028 | 0.1149 |  58.5 min |    2.2 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 1000 |   505515 | 0.1065 |  85.9 min |    2.2 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | **1778** |  1590436 | **0.1046** | 125.5 min |    3.1 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 2579 |  3339820 | 0.1121 | 180.8 min |    3.1 GiB|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization validation curve **(TORUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 2020-02-18T17:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   5 | elapsed: 31.6min remaining: 126.3min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 31.7min remaining: 47.5min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed: 31.8min remaining: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 42.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 42.4min finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1778)\n",
    "param_name='classifier__estimator__C'\n",
    "param_range = np.logspace(-1, 1, 9)  # [0.1,0.1778279410038923, 0.31622776601683794, 0.5623413251903491, 1,\n",
    "                                     # 1.7782794100389228, 3.1622776601683795, 5.623413251903491, 10]\n",
    "param_label = 'C'\n",
    "plot_validation_curve(pl, sampling[FEATURES], dummy_labels, param_name, param_range, param_label,\n",
    "                      cv=5, scoring=multi_multi_log_loss_scorer, n_jobs=-1, verbose=11, xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding fixed `ngram_range=(1,1)`, `n_features=2**20` and `k=1778`, the best is `C=0.5623413251903491`, scoring `0.1041`, elapsed `hundreds of minutes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit & predict **(SUBMIT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting started on 2020-02-18T17:48\n",
      "[Pipeline] .. (step 1 of 2) Processing numeric_selector, total=   0.1s\n",
      "[Pipeline] ........... (step 2 of 2) Processing imputer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   0.9s\n",
      "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   0.9s\n",
      "[Pipeline] ........... (step 3 of 3) Processing reducer, total=  18.8s\n",
      "[Pipeline] ............. (step 1 of 4) Processing union, total=  20.8s\n",
      "[Pipeline] ........ (step 2 of 4) Processing interactor, total=   0.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   1.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9314491ef3b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'0.1-k1778-logistic-regression-C0.31622776601683794'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m to_csv_to_zip(prediction_dir, model_name, pl.predict_proba(holdout),\n\u001b[0;32m     11\u001b[0m               holdout.index, dummy_labels.columns)\n",
      "\u001b[1;32m~\\gdrive\\cursos\\datacamp\\python-data-science\\22-ml-experts-school-budgets-case\\notebooks\\..\\src\\fit_cache.py\u001b[0m in \u001b[0;36mfit_cache\u001b[1;34m(estimator, X, y, folder, modelname)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fitting started on {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimespec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minutes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done: {:.1f} minutes'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saving cache {} ... '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[1;34m\"not %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[1;32m--> 239\u001b[1;33m             for i, column in enumerate(columns))\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dev\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "              classifier__n_jobs = -1,\n",
    "              classifier__estimator__C = 0.5623413251903491,\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1778)\n",
    "\n",
    "model_name = '0.1-k1778-logistic-regression-C0.5623413251903491'\n",
    "pl = fit_cache(pl, sampling[FEATURES], dummy_labels, model_dir, model_name)\n",
    "to_csv_to_zip(prediction_dir, model_name, pl.predict_proba(holdout),\n",
    "              holdout.index, dummy_labels.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ngram_range=(1,1)`, `n_features=2**20`, `k=1778` and `C=1`, elapsed `31.1 minutes`, DrivenData score: `0.4818`  \n",
    "`ngram_range=(1,1)`, `n_features=2**20`, `k=1778` and `C=0.5623413251903491`, elapsed ` minutes`, DrivenData score: `0.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling = multilabel_sample_dataframe(df, y, size = 0.2, min_count = 2, seed = 1)\n",
    "print('Sample size:', sampling.shape[0])                                     # 80055\n",
    "print('Train sizes:', (sampling.shape[0] * train_sizes * .8).astype('int'))  # [12808 25617 38426 51235 64044]\n",
    "print('Test sizes :', (sampling.shape[0] * train_sizes * .2).astype('int'))  # [3202  6404  9606 12808 16011]\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS], prefix_sep='__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('Logistic regression (liblinear)', OneVsRestClassifier(LogisticRegression(solver='liblinear')))#, #fast4small\n",
    "    #('Logistic regression (lbfgs)', OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=200))), #fast4small\n",
    "    #('Logistic regression (sag)', OneVsRestClassifier(LogisticRegression(solver='sag', max_iter=4000)))#, # faster?\n",
    "    #('Logistic regression (saga)', OneVsRestClassifier(LogisticRegression(solver='saga', max_iter=3200))), #fast??\n",
    "    #('Logistic regression (newton-cg)', OneVsRestClassifier(LogisticRegression(solver='newton-cg')))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the`k` parameter space limits for each `ngram_range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_kmax = [len(CountVectorizer(ngram_range=(1,n),\n",
    "                                  dtype='uint8').fit(combine_text_columns(sampling)).vocabulary_)\n",
    "              for n in range(1, 4)] # [2959, 22304, 58861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram1_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[0])), 7).round().astype('int')\n",
    "ngram1_logspace = np.hstack((ngram1_logspace[ngram1_logspace < ngram_kmax[0]], ngram_kmax[0]))\n",
    "ngram1_logspace # [ 316,  562, 1000, 1778, 2959]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,1)],\n",
    "                             'union__text_features__reducer__k': ngram1_logspace}])\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=-1, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| sample | min | jobs | features | ngram | train_sizes |    k | interact | logloss|      time |      peak |\n",
    "| -----: | --: | ---: | -------: | ----: | ----------: | ---: | -------: | -----: |  -------: | --------: |\n",
    "|    0.2 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  316 |    51681 | 0.1132 |  94.0 min |    2.3 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  562 |   161028 | 0.0946 | 142.6 min |    2.3 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 1000 |   505515 | 0.0866 | 194.4 min |    2.3 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | **1778** |  1590436 | **0.0844** | 268.3 min |    3.0 GiB|\n",
    "|    0.1 |   2 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 2959 |  4394130 | 0.0868 | 408.2 min |    4.2 GiB|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization validation curve **(TORUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')), # , max_iter=300)))\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1778)\n",
    "param_name='classifier__estimator__C'\n",
    "param_range = np.logspace(-1, 1, 9)  # [0.1,0.1778279410038923, 0.31622776601683794, 0.5623413251903491, 1,\n",
    "                                     # 1.7782794100389228, 3.1622776601683795, 5.623413251903491, 10]\n",
    "param_label = 'C'\n",
    "plot_validation_curve(pl, sampling[FEATURES], dummy_labels, param_name, param_range, param_label,\n",
    "                      cv=5, scoring=multi_multi_log_loss_scorer, n_jobs=-1, verbose=11, xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding fixed `ngram_range=(1,1)`, `n_features=2**20` and `k=1778`, the best is `C=?`, scoring `?`, elapsed `? minutes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "              classifier__n_jobs = -1,\n",
    "              classifier__estimator__C = 1,\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1778)\n",
    "\n",
    "model_name = '0.2-k1778-logistic-regression-C1'\n",
    "pl = fit_cache(pl, sampling[FEATURES], dummy_labels, model_dir, model_name)\n",
    "to_csv_to_zip(prediction_dir, model_name, pl.predict_proba(holdout),\n",
    "              holdout.index, dummy_labels.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ngram_range=(1,1)`, `n_features=2**20`, `k=1778` and `C=1`, elapsed `33.9 minutes`, DrivenData score: `0.4800`  \n",
    "\n",
    "Fitting started on 2020-02-15T13:32  \n",
    "[Pipeline] .. (step 1 of 2) Processing numeric_selector, total=   0.1s  \n",
    "[Pipeline] ........... (step 2 of 2) Processing imputer, total=   0.0s  \n",
    "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   2.1s  \n",
    "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   1.8s  \n",
    "[Pipeline] ........... (step 3 of 3) Processing reducer, total=  19.0s  \n",
    "[Pipeline] ............. (step 1 of 4) Processing union, total=  23.1s  \n",
    "[Pipeline] ........ (step 2 of 4) Processing interactor, total=   1.2s  \n",
    "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   2.1s  \n",
    "[Pipeline] ........ (step 4 of 4) Processing classifier, total=33.4min  \n",
    "Done: 33.9 minutes  \n",
    "Saving cache 0.2-k1778-logistic-regression-C1 ... Done: 0.0 minutes  \n",
    "Saving CSV...Done: 0.2 minutes  \n",
    "Zipping...Done: 0.2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 120083\n",
      "Train sizes: [19213 38426 57639 76853 96066]\n",
      "Test sizes : [ 4803  9606 14409 19213 24016]\n"
     ]
    }
   ],
   "source": [
    "sampling = multilabel_sample_dataframe(df, y, size = 0.3, min_count = 0, seed = 1)\n",
    "print('Sample size:', sampling.shape[0])                                     # 120083\n",
    "print('Train sizes:', (sampling.shape[0] * train_sizes * .8).astype('int'))  # [19213 38426 57639 76853 96066]\n",
    "print('Test sizes :', (sampling.shape[0] * train_sizes * .2).astype('int'))  # [ 4803  9606 14409 19213 24016]\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS], prefix_sep='__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('Logistic regression (liblinear)', OneVsRestClassifier(LogisticRegression(solver='liblinear')))#, #fast4small\n",
    "    #('Logistic regression (lbfgs)', OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=200))), #fast4small\n",
    "    #('Logistic regression (sag)', OneVsRestClassifier(LogisticRegression(solver='sag', max_iter=4000)))#, # faster?\n",
    "    #('Logistic regression (saga)', OneVsRestClassifier(LogisticRegression(solver='saga', max_iter=3200))), #fast??\n",
    "    #('Logistic regression (newton-cg)', OneVsRestClassifier(LogisticRegression(solver='newton-cg')))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the`k` parameter space limits for each `ngram_range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_kmax = [len(CountVectorizer(ngram_range=(1,n),\n",
    "                                  dtype='uint8').fit(combine_text_columns(sampling)).vocabulary_)\n",
    "              for n in range(1, 4)] # [3122, 24721, 66367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 316,  562, 1000, 1778, 3122])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram1_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[0])), 7).round().astype('int')\n",
    "ngram1_logspace = np.hstack((ngram1_logspace[ngram1_logspace < ngram_kmax[0]], ngram_kmax[0]))\n",
    "ngram1_logspace # [ 316,  562, 1000, 1778, 3122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = ParameterGrid([{'union__text_features__vectorizer__ngram_range': [(1,1)],\n",
    "                             'union__text_features__reducer__k': ngram1_logspace}])\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    pl.set_params(classifier__n_jobs = None)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            sampling[FEATURES], dummy_labels, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=-1, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| sample | min | jobs | features | ngram | train_sizes |    k | interact | logloss|      time |      peak |\n",
    "| -----: | --: | ---: | -------: | ----: | ----------: | ---: | -------: | -----: |  -------: | --------: |\n",
    "|    0.3 |   0 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  316 |    51681 | ??? |  min |  GiB|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization validation curve **(TORUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')), # , max_iter=300)))\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1778)\n",
    "param_name='classifier__estimator__C'\n",
    "param_range = np.logspace(-1, 1, 9)  # [0.1,0.1778279410038923, 0.31622776601683794, 0.5623413251903491, 1,\n",
    "                                     # 1.7782794100389228, 3.1622776601683795, 5.623413251903491, 10]\n",
    "param_label = 'C'\n",
    "plot_validation_curve(pl, sampling[FEATURES], dummy_labels, param_name, param_range, param_label,\n",
    "                      cv=5, scoring=multi_multi_log_loss_scorer, n_jobs=-1, verbose=11, xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding fixed `ngram_range=(1,1)`, `n_features=2**20` and `k=1778`, the best is `C=?`, scoring `?`, elapsed `? minutes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "              classifier__n_jobs = -1,\n",
    "              classifier__estimator__C = 1,\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 1778)\n",
    "\n",
    "model_name = '0.3-k1778-logistic-regression-C1'\n",
    "pl = fit_cache(pl, sampling[FEATURES], dummy_labels, model_dir, model_name)\n",
    "to_csv_to_zip(prediction_dir, model_name, pl.predict_proba(holdout),\n",
    "              holdout.index, dummy_labels.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ngram_range=(1,1)`, `n_features=2**20`, `k=1778` and `C=1`, elapsed `35.8 minutes`, DrivenData score: `0.5095`  \n",
    "Fitting started on 2020-02-18T02:13  \n",
    "[Pipeline] .. (step 1 of 2) Processing numeric_selector, total=   0.1s  \n",
    "[Pipeline] ........... (step 2 of 2) Processing imputer, total=   0.0s  \n",
    "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   2.8s  \n",
    "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   2.7s  \n",
    "[Pipeline] ........... (step 3 of 3) Processing reducer, total=  20.0s  \n",
    "[Pipeline] ............. (step 1 of 4) Processing union, total=  25.7s  \n",
    "[Pipeline] ........ (step 2 of 4) Processing interactor, total=   1.4s  \n",
    "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   3.0s  \n",
    "[Pipeline] ........ (step 4 of 4) Processing classifier, total=35.3min  \n",
    "Done: 35.8 minutes  \n",
    "Saving cache 0.3-k1778-logistic-regression-C1 ... Done: 0.0 minutes  \n",
    "Saving CSV...Done: 0.3 minutes  \n",
    "Zipping...Done: 0.2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% data **(TORUN days)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 400277\n",
      "Train sizes: [ 64044 128088 192132 256177 320221]\n",
      "Test sizes : [16011 32022 48033 64044 80055]\n"
     ]
    }
   ],
   "source": [
    "print('Sample size:', df.shape[0])                                     # 400277\n",
    "print('Train sizes:', (df.shape[0] * train_sizes * .8).astype('int'))  # [64044 128088 192132 256177 320221]\n",
    "print('Test sizes :', (df.shape[0] * train_sizes * .2).astype('int'))  # [16011 32022 48033 64044 80055]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves **(TORUN FROM 1778 ON)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [('Logistic regression (liblinear)', OneVsRestClassifier(LogisticRegression(solver='liblinear')))#, #fast4small\n",
    "              #('Logistic regression (lbfgs)', OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=300))), #, #fast4small\n",
    "              #('Logistic regression (sag)', OneVsRestClassifier(LogisticRegression(solver='sag', max_iter=1100))), # faster?\n",
    "              #('Logistic regression (saga)', OneVsRestClassifier(LogisticRegression(solver='saga', max_iter=3200))), #fast??\n",
    "              #('Logistic regression (newton-cg)', OneVsRestClassifier(LogisticRegression(solver='newton-cg'))) #fast\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the`k` parameter space limits for each `ngram_range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_kmax = [len(CountVectorizer(ngram_range=(1,n),\n",
    "                                  dtype='uint8').fit(combine_text_columns(df)).vocabulary_)\n",
    "              for n in range(1, 4)]  # [3728, 32572, 91308]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram1_logspace = np.logspace(2.5, np.ceil(np.log10(ngram_kmax[0])), 7).round().astype('int')\n",
    "ngram1_logspace = np.hstack((ngram1_logspace[ngram1_logspace < ngram_kmax[0]], ngram_kmax[0]))\n",
    "parameters = ParameterGrid({'union__text_features__reducer__k': [1778,3162,3728], # ngram1_logspace, # [316,562,1000,1778,3162,3728]\n",
    "                            'union__text_features__vectorizer__ngram_range': [(1,1)]})\n",
    "print(datetime.now().isoformat(timespec='minutes'))\n",
    "for title, classifier in classifiers:\n",
    "    pl.set_params(classifier = classifier)\n",
    "    for parameter in parameters:\n",
    "        pl.set_params(**parameter)\n",
    "        plot_learning_curve(pl, ', '.join([title] + [k.split('__')[-1]+'='+str(v) for k,v in parameter.items()]),\n",
    "                            df[FEATURES], y, cv=5, scoring=multi_multi_log_loss_scorer,\n",
    "                            n_jobs=-1, verbose=11, train_sizes=train_sizes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| sample | jobs | features | ngram | train_sizes |    k | interact |log loss|       time |      peak |\n",
    "| -----: | ---: | -------: | ----: | ----------: | ---: | -------: |-------:| ---------: | --------: |\n",
    "|    1.0 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  316 |  1678650 | 0.0834 |  667.9 min |2.8-3.1 GiB|\n",
    "|    1.0 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) |  562 |  1678650 | 0.0686 |  998.1 min |2.8-3.1 GiB|\n",
    "|    1.0 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 1000 |  1678650 | 0.0623 | 1291.2 min |2.8-3.1 GiB|\n",
    "|    1.0 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 1778 |  1678650 | to     |        min |2.8-3.1 GiB|\n",
    "|    1.0 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 3162 |  1678650 | to     |        min |2.8-3.1 GiB|\n",
    "|    1.0 |    4 |  2\\*\\*20 | (1,1) | (0.2, 1, 5) | 3728 |  1678650 | to     |        min |2.8-3.1 GiB|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization validation curve **(TORUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear'))) # ConvergenceWarning\n",
    "param_name='classifier__estimator__C'\n",
    "param_range = np.logspace(-1, 1, 9)  # [0.1,0.1778279410038923, 0.31622776601683794, 0.5623413251903491, 1,\n",
    "                                     # 1.7782794100389228, 3.1622776601683795, 5.623413251903491, 10]\n",
    "param_label = 'C'\n",
    "plot_validation_curve(pl, df[FEATURES], y, param_name, param_range, param_label,\n",
    "                      cv=5, scoring=multi_multi_log_loss_scorer, n_jobs=-1, verbose=11, xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding fixed `ngram_range=(1,1)`, `n_features=2**20` and `k=???`, the best is `C=??`, scoring `0.??`, elapsed `38 minutes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit & predict  **(TO RUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting started on 2020-02-17T21:37\n",
      "[Pipeline] .. (step 1 of 2) Processing numeric_selector, total=   0.3s\n",
      "[Pipeline] ........... (step 2 of 2) Processing imputer, total=   0.1s\n",
      "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   9.3s\n",
      "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   8.5s\n",
      "[Pipeline] ........... (step 3 of 3) Processing reducer, total=  23.1s\n",
      "[Pipeline] ............. (step 1 of 4) Processing union, total=  41.9s\n",
      "[Pipeline] ........ (step 2 of 4) Processing interactor, total=   4.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=  12.5s\n",
      "[Pipeline] ....... (step 4 of 4) Processing classifier, total=198.7min\n",
      "Done: 199.6 minutes\n",
      "Saving cache 1.0-k3728-logistic-regression-C1 ... Done: 1.4 minutes\n",
      "Saving CSV...Done: 0.2 minutes\n",
      "Zipping...Done: 0.2 minutes\n"
     ]
    }
   ],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "              classifier__n_jobs = -1,\n",
    "              classifier__estimator__C = 1,\n",
    "              union__text_features__vectorizer__ngram_range = (1,1),\n",
    "              union__text_features__vectorizer__n_features = 2**20,\n",
    "              union__text_features__reducer__k = 3728)\n",
    "model_name = '1.0-k3728-logistic-regression-C1'\n",
    "pl = fit_cache(pl, df[FEATURES], y, model_dir, model_name)\n",
    "to_csv_to_zip(prediction_dir, model_name, pl.predict_proba(holdout), holdout.index, y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saved results for sample 1.0 k 3728 C 1: DrivenData 0.5145**  \n",
    "Fitting started on 2020-02-17T21:37  \n",
    "[Pipeline] .. (step 1 of 2) Processing numeric_selector, total=   0.3s  \n",
    "[Pipeline] ........... (step 2 of 2) Processing imputer, total=   0.1s  \n",
    "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   9.3s  \n",
    "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   8.5s  \n",
    "[Pipeline] ........... (step 3 of 3) Processing reducer, total=  23.1s  \n",
    "[Pipeline] ............. (step 1 of 4) Processing union, total=  41.9s  \n",
    "[Pipeline] ........ (step 2 of 4) Processing interactor, total=   4.7s  \n",
    "[Pipeline] ............ (step 3 of 4) Processing scaler, total=  12.5s  \n",
    "[Pipeline] ....... (step 4 of 4) Processing classifier, total=198.7min  \n",
    "Done: 199.6 minutes  \n",
    "Saving cache 1.0-k3728-logistic-regression-C1 ... Done: 1.4 minutes  \n",
    "Saving CSV...Done: 0.2 minutes  \n",
    "Zipping...Done: 0.2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saved results for sample 1.0 k 1778 C 1: DrivenData 0.5173**  \n",
    "Fitting started on 2020-02-17T18:28  \n",
    "[Pipeline] .. (step 1 of 2) Processing numeric_selector, total=   0.3s  \n",
    "[Pipeline] ........... (step 2 of 2) Processing imputer, total=   0.1s  \n",
    "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   9.6s  \n",
    "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   9.2s  \n",
    "[Pipeline] ........... (step 3 of 3) Processing reducer, total=  23.6s  \n",
    "[Pipeline] ............. (step 1 of 4) Processing union, total=  43.2s  \n",
    "[Pipeline] ........ (step 2 of 4) Processing interactor, total=   4.9s  \n",
    "[Pipeline] ............ (step 3 of 4) Processing scaler, total=  11.4s  \n",
    "[Pipeline] ....... (step 4 of 4) Processing classifier, total=147.8min  \n",
    "Done: 148.8 minutes  \n",
    "Saving cache 1.0-k1778-logistic-regression-C1 ... Done: 0.0 minutes  \n",
    "Saving CSV...Done: 0.2 minutes  \n",
    "Zipping...Done: 0.2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ngram_range=(1,1)`, `n_features=2**20`, `k=1778` and `C=1`, elapsed `148.8 minutes`, DrivenData score: `0.5173`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter optimizations and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.8% training, 0.2% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = multilabel_sample_dataframe(df, y, size = 0.01, min_count = 7, seed = 1)\n",
    "print('Sample size:', sampling.shape[0])\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS], prefix_sep='__')\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(sampling[FEATURES], dummy_labels,\n",
    "                                                               size = 0.2, min_count = 1, seed=1)\n",
    "print('Train size`:', y_train.shape[0])\n",
    "print('Test size  :', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "parameters = {'union__text_features__reducer__k' : np.logspace(2, 3, 5).round().astype('int'),\n",
    "              'classifier__estimator__C' : np.logspace(0, 2, 3)} #[1 10 100]\n",
    "grid = GridSearchCV(estimator = pl,\n",
    "                    n_jobs = -1,\n",
    "                    param_grid = parameters,\n",
    "                    cv = 5,\n",
    "                    scoring = {'logloss' : multi_multi_log_loss_scorer},\n",
    "                    refit = 'logloss',\n",
    "                    verbose=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '0.008-all-features-gridsearch-logistic-regression'\n",
    "grid = fit_cache(grid, X_train, y_train, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 15 candidates, totalling 75 fits, took ~120 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Time refitting best model on whole data  : {:.1f} minutes'.format(grid.refit_time_ / 60))\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results = results.rename(columns={'param_classifier__estimator__C':'C',\n",
    "                                  'param_union__text_features__reducer__k':'k',\n",
    "                                  'split0_test_multi_multi_log_loss':'split0_logloss',\n",
    "                                  'split1_test_multi_multi_log_loss':'split1_logloss',\n",
    "                                  'split2_test_multi_multi_log_loss':'split2_logloss',\n",
    "                                  'split3_test_multi_multi_log_loss':'split3_logloss',\n",
    "                                  'split4_test_multi_multi_log_loss':'split4_logloss',\n",
    "                                  'mean_test_multi_multi_log_loss':'mean_test_logloss',\n",
    "                                  'std_test_multi_multi_log_loss':'std_test_logloss',\n",
    "                                  'rank_test_multi_multi_log_loss':'rank_test_logloss'\n",
    "                                 })\n",
    "results[['C', 'k', 'mean_test_logloss', 'std_test_logloss','rank_test_logloss', 'mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "         'std_score_time']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "#pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')),\n",
    "#              union__text_features__reducer__k = 562,\n",
    "#              classifier__estimator__C = 1)\n",
    "#cross_val_score(pl, X_train, y_train,\n",
    "#                scoring = multi_multi_log_loss_scorer,\n",
    "#                cv = 5,\n",
    "#                n_jobs = -1,\n",
    "#                verbose=11).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Training data score        : {}\".format(grid.score(X_train, y_train)))  # Takes some minutes\n",
    "y_pred = grid.predict(X_train)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_train, y_pred, target_names=y_train.columns, output_dict=True)).transpose()\n",
    "report, summary = report[:-4].sort_values('f1-score', ascending=False), report[-4:]\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Testing data score         : {}\".format(grid.score(X_test, y_test)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_test)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_test, y_pred, target_names=y_test.columns, output_dict=True)).transpose()\n",
    "report, summary = report[:-4].sort_values('f1-score', ascending=False), report[-4:]\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score breakdown checking\n",
    "y_pred = grid.predict_proba(X_test)  # Takes some minutes\n",
    "pd.DataFrame(multi_multi_log_loss(y_test, y_pred, class_column_indices=cci, averaged=False), index=LABELS,\n",
    "             columns=['multi-multi log loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "y_pred = grid.predict_proba(holdout)\n",
    "print('Elapsed: {:.1f} minutes'.format(np.floor(time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv_zip(prediction_dir, model_name, y_pred, holdout.index, y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8% training, 2% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = multilabel_sample_dataframe(df, y, size = 0.1, min_count = 2, seed = 1)\n",
    "print('Sample size:', sampling.shape[0])\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS], prefix_sep='__')\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(sampling[FEATURES], dummy_labels,\n",
    "                                                               size = 0.2, min_count = 0, seed=1)\n",
    "print('Train size`:', y_train.shape[0])\n",
    "print('Test size  :', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "parameters = {'union__text_features__reducer__k' : np.logspace(2, 3, 5).round().astype('int'),\n",
    "              'classifier__estimator__C' : np.logspace(0, 2, 3)} #[1 10 100]\n",
    "grid = GridSearchCV(estimator = pl,\n",
    "                    n_jobs = -1,\n",
    "                    param_grid = parameters,\n",
    "                    cv = 5,\n",
    "                    scoring = {'logloss' : multi_multi_log_loss_scorer},\n",
    "                    refit = 'logloss',\n",
    "                    verbose=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = '0.08-all-features-gridsearch-logistic-regression'\n",
    "grid = fit_cache(grid, X_train, y_train, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 15 candidates, totalling 75 fits, took 392 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time refitting best model on whole data  : {:.0f} minutes'.format(grid.refit_time_ / 60))\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results = results.rename(columns={'param_classifier__estimator__C':'C',\n",
    "                                  'param_union__text_features__reducer__k':'k'})\n",
    "results[['C', 'k', 'mean_test_logloss', 'std_test_logloss','rank_test_logloss', 'mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "         'std_score_time']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(union__text_features__reducer__k = 1000,\n",
    "              classifier__estimator__C = 0.1)\n",
    "cross_val_score(pl, X_train, y_train,\n",
    "                scoring = multi_multi_log_loss_scorer,\n",
    "                cv = 5,\n",
    "                n_jobs = -1,\n",
    "                verbose=11).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Training data score        : {}\".format(grid.score(X_train, y_train)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_train)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_train, y_pred, target_names=y_train.columns, output_dict=True)).transpose()\n",
    "summary = report[:-4].sort_values('f1-score', ascending=False)\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Testing data score         : {}\".format(grid.score(X_test, y_test)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_test)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_test, y_pred, target_names=y_test.columns, output_dict=True)).transpose()\n",
    "report, summary = report[:-4].sort_values('f1-score', ascending=False), report[-4:]\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score checking\n",
    "y_pred = grid.predict_proba(X_test)  # Takes some minutes\n",
    "pd.DataFrame(multi_multi_log_loss(y_test, y_pred, class_column_indices=cci, averaged=False), index=LABELS,\n",
    "             columns=['multi-multi log loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "y_pred = grid.predict_proba(holdout)\n",
    "print('Elapsed: {:.1f} minutes'.format(np.floor(time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv_zip(prediction_dir, model_name, y_pred, holdout.index, y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16% training, 4% testing **(TO RUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = multilabel_sample_dataframe(df, y, size = 0.2, min_count = 2, seed = 1)\n",
    "print('Sample size:', sampling.shape[0])\n",
    "dummy_labels = pd.get_dummies(sampling[LABELS], prefix_sep='__')\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(sampling[FEATURES], dummy_labels,\n",
    "                                                               size = 0.2, min_count = 0, seed=1)\n",
    "print('Train size`:', y_train.shape[0])\n",
    "print('Test size  :', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "parameters = {'union__text_features__reducer__k' : np.logspace(2, 3, 5).round().astype('int'),\n",
    "              'classifier__estimator__C' : np.logspace(0, 2, 3)} #[1 10 100]\n",
    "grid = GridSearchCV(estimator = pl,\n",
    "                    n_jobs = -1,\n",
    "                    param_grid = parameters,\n",
    "                    cv = 5,\n",
    "                    scoring = {'logloss' : multi_multi_log_loss_scorer},\n",
    "                    refit = 'logloss',\n",
    "                    verbose=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = '0.016-all-features-gridsearch-logistic-regression'\n",
    "grid = fit_cache(grid, X_train, y_train, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time refitting best model on whole data  : {:.0f} minutes'.format(grid.refit_time_ / 60))\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results = results.rename(columns={'param_classifier__estimator__C':'C',\n",
    "                                  'param_union__text_features__reducer__k':'k'})\n",
    "results[['C', 'k', 'mean_test_logloss', 'std_test_logloss','rank_test_logloss', 'mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "         'std_score_time']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Training data score        : {}\".format(grid.score(X_train, y_train)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_train)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_train, y_pred, target_names=y_train.columns, output_dict=True)).transpose()\n",
    "summary = report[:-4].sort_values('f1-score', ascending=False)\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Testing data score         : {}\".format(grid.score(X_test, y_test)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_test)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_test, y_pred, target_names=y_test.columns, output_dict=True)).transpose()\n",
    "report, summary = report[:-4].sort_values('f1-score', ascending=False), report[-4:]\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score checking\n",
    "y_pred = grid.predict_proba(X_test)  # Takes some minutes\n",
    "pd.DataFrame(multi_multi_log_loss(y_test, y_pred, class_column_indices=cci, averaged=False), index=LABELS,\n",
    "             columns=['multi-multi log loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "y_pred = grid.predict_proba(holdout)\n",
    "print('Elapsed: {:.1f} minutes'.format(np.floor(time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv_zip(prediction_dir, model_name, y_pred, holdout.index, y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80% training, 20% testing **(TO RUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = multilabel_train_test_split(df[FEATURES], y, size=0.2, min_count=0, seed=1)\n",
    "print('Train size`:', y_train.shape[0])\n",
    "print('Test size  :', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "parameters = {'union__text_features__reducer__k' : np.logspace(2, 3, 5).round().astype('int'),\n",
    "              'classifier__estimator__C' : np.logspace(0, 2, 3)} #[1 10 100]\n",
    "grid = GridSearchCV(estimator = pl,\n",
    "                    n_jobs = -1,\n",
    "                    param_grid = parameters,\n",
    "                    cv = 5,\n",
    "                    scoring = {'logloss' : multi_multi_log_loss_scorer},\n",
    "                    refit = 'logloss',\n",
    "                    verbose=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '0.8-all-features-gridsearch-logistic-regression'\n",
    "grid = fit_cache(grid, X_train, y_train, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time refitting best model on whole data  : {:.0f} minutes'.format(grid.refit_time_ / 60))\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results = results.rename(columns={'param_classifier__estimator__C':'C',\n",
    "                                  'param_union__text_features__reducer__k':'k'})\n",
    "results[['C', 'k', 'mean_test_logloss', 'std_test_logloss','rank_test_logloss', 'mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "         'std_score_time']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Training data score        : {}\".format(grid.score(X_train, y_train)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_train)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_train, y_pred, target_names=y_train.columns, output_dict=True)).transpose()\n",
    "summary = report[:-4].sort_values('f1-score', ascending=False)\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Testing data score         : {}\".format(grid.score(X_test, y_test)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_test)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_test, y_pred, target_names=y_test.columns, output_dict=True)).transpose()\n",
    "report, summary = report[:-4].sort_values('f1-score', ascending=False), report[-4:]\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score checking\n",
    "y_pred = grid.predict_proba(X_test)  # Takes some minutes\n",
    "pd.DataFrame(multi_multi_log_loss(y_test, y_pred, class_column_indices=cci, averaged=False), index=LABELS,\n",
    "             columns=['multi-multi log loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "y_pred = grid.predict_proba(holdout)\n",
    "print('Elapsed: {:.1f} minutes'.format(np.floor(time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv_zip(prediction_dir, model_name, y_pred, holdout.index, y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100% training, 0% testing **(TO RUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=1) # Ensure iid samples because CVGridSearch/KFold doesn't shuffle folding data\n",
    "X_train = df[FEATURES]\n",
    "y_train = pd.get_dummies(df[LABELS], prefix_sep='__')\n",
    "del, X_test, y_test\n",
    "print('Train size`:', y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_params(classifier = OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "parameters = {'union__text_features__reducer__k' : np.logspace(2, 3, 5).round().astype('int'),\n",
    "              'classifier__estimator__C' : np.logspace(0, 2, 3)} #[1 10 100]\n",
    "grid = GridSearchCV(estimator = pl,\n",
    "                    n_jobs = -1,\n",
    "                    param_grid = parameters,\n",
    "                    cv = 5,\n",
    "                    scoring = {'logloss' : multi_multi_log_loss_scorer},\n",
    "                    refit = 'logloss',\n",
    "                    verbose=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '1.0-all-features-gridsearch-logistic-regression'\n",
    "grid = fit_cache(grid, X_train, y_train, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time refitting best model on whole data  : {:.0f} minutes'.format(grid.refit_time_ / 60))\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results = results.rename(columns={'param_classifier__estimator__C':'C',\n",
    "                                  'param_union__text_features__reducer__k':'k'})\n",
    "results[['C', 'k', 'mean_test_logloss', 'std_test_logloss','rank_test_logloss', 'mean_fit_time', 'std_fit_time', 'mean_score_time',\n",
    "         'std_score_time']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean cross-validated score : {}'.format(grid.best_score_))\n",
    "print(\"Training data score        : {}\".format(grid.score(X_train, y_train)))  # Takes some minutes\n",
    "\n",
    "y_pred = grid.predict(X_train)  # Takes some minutes\n",
    "report = pd.DataFrame(classification_report(y_train, y_pred, target_names=y_train.columns, output_dict=True)).transpose()\n",
    "summary = report[:-4].sort_values('f1-score', ascending=False)\n",
    "display(report)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training score checking\n",
    "y_pred = grid.predict_proba(X_train)  # Takes some minutes\n",
    "pd.DataFrame(multi_multi_log_loss(y_train, y_pred, class_column_indices=cci, averaged=False), index=LABELS,\n",
    "             columns=['multi-multi log loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "y_pred = grid.predict_proba(holdout)\n",
    "print('Elapsed: {:.1f} minutes'.format(np.floor(time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv_zip(prediction_dir, model_name, y_pred, holdout.index, y.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "543.2px",
    "left": "36px",
    "top": "90.6px",
    "width": "364.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 651.1999999999999,
   "position": {
    "height": "40px",
    "left": "642px",
    "right": "20px",
    "top": "-5px",
    "width": "602.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
